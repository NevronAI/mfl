Did you ever wonder how [Google's Gboard](https://arxiv.org/abs/1811.03604) keyboard can predict your next word without sending your messages to Google's servers? Or how [Apple's Siri](https://www.technologyreview.com/2019/12/11/131629/apple-ai-personalizes-siri-federated-learning/) can wake up to the "Hey Siri" command without sending your voice to Apple's servers? Or how [hospitals](https://www.nature.com/articles/s41746-020-00323-1) can solve the problem of data sharing and data privacy to train a machine learning model that can predict the risk of a patient to develop a disease? The answer to all these questions is Federated Learning!

## The Problem

The common problem in all these cases is that the data are generated at multiple, distributed locations. In many situations, these data cannot be exported from their original location for several different reasons:

- üë®‚Äç‚öñÔ∏è **Regulatory**: Data types like electronic health records are heavily regulated and protected, limiting the ability of data sharing and performing meaningful data analysis. Similarly, other data types, like industrial data (e.g., accident or safety data), may not be possible to be shared with other business entities due to competitiveness reasons.

- üîí **Privacy**: Data that are generated by individuals (e.g., mobile phones, smartwatches, etc.) are considered private and cannot be shared with other entities. This is also the case for data that are generated by organizations (e.g., hospitals, banks, etc.) and are considered proprietary.

- üóÇÔ∏è **Data Volume and Velocity** : When the number of edge devices (e.g., mobile phones, smartwatches, sensors, etc.) or the number of organizations (e.g., hospitals, banks, etc.) is large, then the data volume and velocity can be overwhelming. This makes the data transfer and aggregation process infeasible.

Whether the data are considered private, proprietary, or regulated, the common problem is that they cannot be transferred from their original location to a centralized location for machine learning modeling. So, if we cannot transfer the data to the machine learning model, then we need to bring the machine learning model to the data. This is the main idea behind Federated Learning.

## A Bit of History

The history of federated learning is short but very promising. Federated Learning was introduced in the seminal paper [Communication-efficient learning of deep networks from decentralized data](https://proceedings.mlr.press/v54/mcmahan17a/mcmahan17a.pdf) by Google in 2017. The authors proposed a framework that allows to train a machine learning model across multiple data sources without the need to transfer the data to a centralized location. The framework was initially applied to train a language model across mobile phones. The model was trained locally on each phone and only the model parameters were transferred to a centralized location for aggregation. The aggregated model was then sent back to the phones to be used for inference.


<div align="center">
  <picture>
  <source media="(prefers-color-scheme: light)" srcset="https://docs.nevron.ai/img/light/CentralizedMachineLearning-02.webp" width="40%">
  <img alt="MetisFL Components Overview" src="https://docs.nevron.ai/img/dark/CentralizedMachineLearning-01.webp" width="40%">
  </picture>
  <picture>
  <source media="(prefers-color-scheme: light)" srcset="https://docs.nevron.ai/img/light/FederatedMachineLearning-02.webp" width="45%">
  <img alt="MetisFL Components Overview" src="https://docs.nevron.ai/img/dark/FederatedMachineLearning-01.webp" width="45%">
</picture>
</div>

<!-- ## Is FL Needed?

Given the technical challenges of federated learning, one may wonder if federated learning is really needed. How about each individual organization or edge devices trains and uses their own model without carrying about the models of other organizations or edge devices? The answer is no! Federated learning is needed for the following reasons:

A couple of studies (see also [[Mcmahan 2017](#mcmahan2017), [Kairouz 2019](#kairouz2019), [Rieke 2020](#yang2020)]) have shown that Federated Machine Learning models can achieve comparable performance as centralized models across various challenging learning tasks.

Here, we discuss a prototypical use case where the task is to predict the chronological brain age from brain structural MRI scans. The difference between the predicted and true chronological brain age values is a phenotype related to aging and brain disease. This task is commonly referred to as BrainAGE.

To demonstrate the significance and promise of Federated Learning in the figure below, we compare the performance of a federated CNN model trained (using MetisFL) across 8 data sources/silos to its centralized and siloed counterparts. By centralized, we refer to the model trained over all pooled data and by siloed to the model trained at each source independently. For more details and more thorough evaluation across more challenging federated environments please also refer to [[Stripelis 2022](#stripelis2022)].

<div align="center">
<img
style={{margin: '0 0 0 30px', display: 'block'}}
src="../img/CentralizedvsSiloedvsFederated-BrainAGE.webp"  alt="Centralized vs. Federated vs. Siloed BrainAGE models." />
</div>

:::info
For this illustrative case the data distribution for the federated environment is Independent and Identically Distributed (IID), the federated training is conducted over 40 federation rounds using Federated Average (FedAvg). [[Stripelis 2022](#stripelis2022)] has a similar evaluation for Non-IID settings.
::: -->

## Federated Topologies

The physical location, the computational capabilities and the data distribution of learners (nodes, clients) participating in the training of a federated model can greatly affect the communication architecture and workflow of a federated learning environment. The figure visualizes the differences across the centralized, decentralized and hierarchical topologies.

<div align="center">  
  <picture>
  <source media="(prefers-color-scheme: light)" srcset="https://docs.nevron.ai/img/light/FederatedLearningTopologies-02.webp" width="500px">
  <img alt="Federated Learning Topologies" src="https://docs.nevron.ai/img/dark/FederatedLearningTopologies-01.webp" width="500px">
</picture>
</div>

Depending on the requirements of the workflow different topologies can be constructed:

- The **centralized** (star-shaped) topology is the most widely used and consists of a centralized controller (coordinator, aggregator) and a collection of learners. The controller orchestrates the execution of the federated learning workflow and it is responsible to periodically aggregate the local models of the participating learners.

- In the case where a centralized controller is absent, then learners can communicate directly to each other and aggregate other learners' local models. This federated learning topology is known as **decentralized** or **peer-to-peer**.

- When multiple controllers/coordinators and a set of cloud-networks and/or sub-aggregators are combined in a more hierarchical-like structure, then we can identify the **hierarchical** (head aggregator with sub-aggregators) or the **hybrid hierarchical** (different combinations of aggregation and training nodes) federated learning topologies.

The centralized topology is most of the time seen in federated environments where participating learners are large organizations/institutions, a.k.a. cross-silo federated settings. However, this topology is also applicable to federated environments consisting of edge devices, a.k.a cross-device federated settings. In the case of decentralized and hierarchical topologies, these topologies are more applicable to cross-device settings.

## Federated Training

Federated training differs from centralized training as it involves the coordination an communication between multiple parties. The main steps of a federated training process are the following:

### Initialize global model

This step is the same as in centralized training. The global model is initialized either randomly or using a pre-trained model. One subtle difference with centralized training is that we need to ensure that the initial global model is the same across all learners. While this is not an issue if we are using a pre-trained model, it can be an problem if we are using a randomly initialized model. In the later case, we can either ensure that all learners use the same random seed or we can randomly pick one learner to initialize the global model and then send it to all other learners.

### Select and Schedule Learners

The next step is for the Controller to select a subset of the Learners to send the first training task to. The selection of the learners can be done randomly or using a more complex selection algorithm. The selection algorithm can be based on the performance of the learners, the computational capabilities of the learners, the data distribution of the learners, etc. After the selection, the Controller sends the training task to the selected learners, either in a synchronous or asynchronous manner. In the synchronous case, all learners are sent the training task at the same time. In the asynchronous case, each learner is sent the next training task when it finished the previous one. In the semi-synchronous case, each learner receives the training task when it finishes the previous one, but the next task is selected based on the running time of the previous tasks.

### Train on local data

This step is similar to the centralized training. Each learner trains the global model on its local data using the model weights sent by the Controller. In addition to those model weights, the Controller may send additional hyperparameters to the learner, e.g., the learning rate, the number of epochs, the batch size, the optimizer, etc. The reason for having the Controller send the hyperparameters is to allow for more sophisticated scheduling algorithms. For example, the Controller can send different hyperparameters to different learners based on their computational capabilities.

### Send and Aggregate Weights

After the distributed training on each local dataset, the learners have slightly different local models. The next step is for the learners to send their local models to the Controller, which then aggregates them using an aggregation rule and sends the aggregated model back to the learners. The aggregation rule can be a simple average of the local models or a more complex aggregation rule, e.g., weighted average, FedAvg, FedProx, etc. The aggregation rule usually takes into account the number of samples used to train each local model. For example, if one learner has 1000 samples and another learner has 100 samples, then the aggregated model should be more influenced by the first learner. This is because more sample means better gradient estimation, therefore, we should give more weight to the model trained on the 1000 samples.

### Select and Schedule Learners

After the model aggregation the Controller selects the next subset of learners to send the next training task to. The selection and scheduling of the learners can be done in the same way as in the previous step. The only difference is that the Controller will now send the aggregated model to the selected learners instead of the initial global model. This process constitutes one `federated round` and is repeated until the global model converges or until a predefined termination criterion is met. Common termination criteria are the number of federated rounds, the runtime of the experiment or a metric threshold.

## üöÄ Next steps

Congratulations üëè. You have completed the introduction and know the basics of federated learning. The best way to continue learning is to dive into the quick start guide and start training your first federated models! But first, make sure you have installed MetisFL!